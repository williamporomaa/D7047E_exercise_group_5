{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a206741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: xpu\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 🟢 Cell 1 – Imports, seed, device\n",
    "# -----------------------------------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils as tvu\n",
    "\n",
    "import numpy as np\n",
    "import math, random, os, time, itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "import intel_extension_for_pytorch as ipex\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Uncomment below if you want experiment tracking\n",
    "# import wandb\n",
    "# wandb.init(project=\"mnist_diffusion\", config={\"task\": \"2.5\"})\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); \n",
    "np.random.seed(SEED); \n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ── Device selection ─────────────────────────────────────────\n",
    "# IPEX registers 'xpu' for Intel GPUs via DPC++\n",
    "if hasattr(torch, \"xpu\") and torch.xpu.is_available():\n",
    "    DEVICE = torch.device(\"xpu\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6688a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 🔧 Cell X – Hyperparameters (centralized)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# Random seed for reproducibility\n",
    "SEED          = 42\n",
    "\n",
    "# Device\n",
    "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Data\n",
    "BATCH_SIZE    = 128\n",
    "NUM_WORKERS   = 2            # keep parallelism now that everything is pickle-safe\n",
    "NUM_CLASSES   = 10\n",
    "IMG_SHAPE     = (1, 28, 28)\n",
    "\n",
    "# Diffusion process\n",
    "T             = 200          # Number of timesteps\n",
    "BETA_SCHEDULE = \"cosine\"     # Options: \"linear\", \"cosine\", etc.\n",
    "\n",
    "# Model architecture\n",
    "BASE_CHANNELS = 64           # \"base_c\" in UNet\n",
    "TIME_EMB_DIM  = 128          # Dimensionality of time (and label) embeddings\n",
    "\n",
    "# Optimization / Training\n",
    "LR            = 2e-4         # Learning rate for AdamW\n",
    "WEIGHT_DECAY  = 0.0          # If you wish to add weight decay\n",
    "N_EPOCHS      = 15           # Number of training epochs\n",
    "\n",
    "# Sampling\n",
    "DDIM_ETA      = 0.0          # η parameter for DDIM sampler (0 → deterministic)\n",
    "\n",
    "# Auxiliary classifier (for evaluation)\n",
    "CLF_LR        = 1e-3         # Learning rate for TinyCNN classifier\n",
    "CLF_EPOCHS    = 2            # Quick training epochs on MNIST\n",
    "\n",
    "# (Optionally) WandB logging\n",
    "USE_WANDB     = False\n",
    "WANDB_PROJECT = \"mnist_diffusion\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5802096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 🟢 Cell 2 – Dataset & label-conditioning helper (patched)\n",
    "# -----------------------------------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))   # built-in, pickle-safe\n",
    "])\n",
    "\n",
    "train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_ds  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "316594be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 🟢 Cell 3 – Diffusion schedule utilities\n",
    "# -----------------------------------------------------------\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    Cosine schedule from Nichol & Dhariwal 2021 (DDPM++).\n",
    "    Returns a tensor of betas with shape [T].\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi / 2)**2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clamp(betas, 1e-8, 0.999)\n",
    "\n",
    "T = 200               # diffusion steps (kept small for MNIST)\n",
    "betas  = cosine_beta_schedule(T).to(DEVICE)\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod     = torch.cumprod(alphas, dim=0)\n",
    "sqrt_alphas_cumprod= torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_acp = torch.sqrt(1 - alphas_cumprod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fdaf204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 🟢 Cell 4 – Positional & label embeddings\n",
    "# -----------------------------------------------------------\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard 1-D sinusoidal embeddings for timestep t (shape [B]).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half  = self.dim // 2\n",
    "        emb   = math.log(10000) / (half - 1)\n",
    "        emb   = torch.exp(torch.arange(half, device=device) * -emb)\n",
    "        emb   = t[:, None] * emb[None, :]\n",
    "        emb   = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "        return emb\n",
    "\n",
    "class LabelEmbedding(nn.Module):\n",
    "    def __init__(self, num_classes, dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_classes, dim)\n",
    "    def forward(self, y):\n",
    "        return self.emb(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0a421c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 🟢 Cell 5 – A *tiny* UNet denoiser  (patched channel counts)\n",
    "# -----------------------------------------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_c: int, out_c: int, time_emb_dim: int):\n",
    "        \"\"\"\n",
    "        Simple ResNet block with FiLM-style time/label conditioning.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, out_c)\n",
    "        )\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, 3, padding=1),\n",
    "            nn.GroupNorm(8, out_c),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_c, out_c, 3, padding=1),\n",
    "            nn.GroupNorm(8, out_c)\n",
    "        )\n",
    "        self.res_conv = nn.Conv2d(in_c, out_c, 1) if in_c != out_c else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.block[0](x)          # first conv\n",
    "        h = self.block[1](h)          # GN\n",
    "        h = h + self.mlp(t_emb)[:, :, None, None]   # add conditioning\n",
    "        h = self.block[2:](h)         # SiLU + conv + GN\n",
    "        return h + self.res_conv(x)   # residual add\n",
    "\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet-ish network that predicts ε for DDPM, conditioned on timestep + class label.\n",
    "    Channel dimensions have been fixed to avoid concat-mismatch errors.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_ch=1, base_c=64, time_emb_dim=128, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---------- embeddings ----------\n",
    "        self.time_emb = nn.Sequential(\n",
    "            SinusoidalPosEmb(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim * 4, time_emb_dim)\n",
    "        )\n",
    "        self.label_emb = LabelEmbedding(num_classes, time_emb_dim)\n",
    "\n",
    "        # ---------- down path ----------\n",
    "        self.down1 = ResidualBlock(img_ch, base_c, time_emb_dim)        # 1  → 64\n",
    "        self.pool1 = nn.MaxPool2d(2)                                     # 28→14\n",
    "        self.down2 = ResidualBlock(base_c, base_c * 2, time_emb_dim)    # 64 →128\n",
    "        self.pool2 = nn.MaxPool2d(2)                                     # 14→7\n",
    "\n",
    "        # ---------- bottleneck ----------\n",
    "        self.mid = ResidualBlock(base_c * 2, base_c * 2, time_emb_dim)   # 128→128\n",
    "\n",
    "        # ---------- up path ----------\n",
    "        self.up2  = nn.ConvTranspose2d(base_c * 2, base_c, 2, stride=2)  # 7→14, 128→64\n",
    "        # [u2 (64) ⊕ d2 (128)] = 192 channels\n",
    "        self.res2 = ResidualBlock(base_c * 3, base_c, time_emb_dim)      # 192→64\n",
    "\n",
    "        self.up1  = nn.ConvTranspose2d(base_c, base_c // 2, 2, stride=2) # 14→28, 64→32\n",
    "        # [u1 (32) ⊕ d1 (64)] = 96 channels\n",
    "        self.res1 = ResidualBlock(base_c + base_c // 2, base_c // 2, time_emb_dim)  # 96→32\n",
    "\n",
    "        self.out_conv = nn.Conv2d(base_c // 2, img_ch, 1)                # 32→1\n",
    "\n",
    "    def forward(self, x, t, y):\n",
    "        \"\"\"\n",
    "        x : noisy image  [B, 1, 28, 28]\n",
    "        t : timestep     [B]\n",
    "        y : digit label  [B]\n",
    "        \"\"\"\n",
    "        # --- combine timestep + label embeddings ---\n",
    "        cond_emb = self.time_emb(t) + self.label_emb(y)\n",
    "\n",
    "        # --- encoder ---\n",
    "        d1 = self.down1(x, cond_emb)             # 64 ch\n",
    "        d2 = self.down2(self.pool1(d1), cond_emb)# 128 ch\n",
    "\n",
    "        # --- bottleneck ---\n",
    "        m  = self.mid(self.pool2(d2), cond_emb)  # 128 ch\n",
    "\n",
    "        # --- decoder + skip 2 ---\n",
    "        u2 = self.up2(m)                         # 64 ch, 14×14\n",
    "        u2 = torch.cat([u2, d2], dim=1)          # 192 ch\n",
    "        u2 = self.res2(u2, cond_emb)             # 64 ch\n",
    "\n",
    "        # --- decoder + skip 1 ---\n",
    "        u1 = self.up1(u2)                        # 32 ch, 28×28\n",
    "        u1 = torch.cat([u1, d1], dim=1)          # 96 ch\n",
    "        u1 = self.res1(u1, cond_emb)             # 32 ch\n",
    "\n",
    "        return self.out_conv(u1)                 # 1 ch (ε̂)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06c48754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 🟢 Cell 6 – Forward-diffusion helper + loss\n",
    "# -----------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def q_sample(x0, t, noise=None):\n",
    "    \"\"\"Diffuse the clean image x0 to x_t via q(x_t | x0).\"\"\"\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x0)\n",
    "    sqrt_acp     = sqrt_alphas_cumprod[t][:, None, None, None]\n",
    "    sqrt_1m_acp  = sqrt_one_minus_acp[t][:, None, None, None]\n",
    "    return sqrt_acp * x0 + sqrt_1m_acp * noise\n",
    "\n",
    "def diffusion_loss(model, x0, y):\n",
    "    \"\"\"Simplified DDPM objective: predict ε directly (MSE).\"\"\"\n",
    "    B = x0.size(0)\n",
    "    t = torch.randint(0, T, (B,), device=DEVICE, dtype=torch.long)\n",
    "    noise = torch.randn_like(x0)\n",
    "    x_t   = q_sample(x0, t, noise)\n",
    "    ε_pred = model(x_t, t, y)\n",
    "    return F.mse_loss(ε_pred, noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "536d252e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 01: 100%|██████████| 469/469 [04:46<00:00,  1.64it/s, loss=0.1312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: mean loss 0.1312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 02: 100%|██████████| 469/469 [04:51<00:00,  1.61it/s, loss=0.0616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: mean loss 0.0616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 03: 100%|██████████| 469/469 [04:44<00:00,  1.65it/s, loss=0.0535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: mean loss 0.0535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 04: 100%|██████████| 469/469 [05:06<00:00,  1.53it/s, loss=0.0502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: mean loss 0.0502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 05: 100%|██████████| 469/469 [05:08<00:00,  1.52it/s, loss=0.0473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: mean loss 0.0473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 06: 100%|██████████| 469/469 [04:51<00:00,  1.61it/s, loss=0.0459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: mean loss 0.0459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 07: 100%|██████████| 469/469 [04:49<00:00,  1.62it/s, loss=0.0446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: mean loss 0.0446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 08: 100%|██████████| 469/469 [04:50<00:00,  1.62it/s, loss=0.0443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: mean loss 0.0443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 09: 100%|██████████| 469/469 [04:44<00:00,  1.65it/s, loss=0.0434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: mean loss 0.0434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 469/469 [04:46<00:00,  1.64it/s, loss=0.0429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: mean loss 0.0429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 469/469 [04:54<00:00,  1.59it/s, loss=0.0423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: mean loss 0.0423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 469/469 [05:14<00:00,  1.49it/s, loss=0.0419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: mean loss 0.0419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 469/469 [05:07<00:00,  1.53it/s, loss=0.0417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: mean loss 0.0417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 469/469 [04:49<00:00,  1.62it/s, loss=0.0416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: mean loss 0.0416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 469/469 [04:45<00:00,  1.64it/s, loss=0.0414]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: mean loss 0.0414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 🟢 Cell 7 – Optimiser & training loop (IPEX on XPU, FP32 only)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "model = SimpleUNet(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "model, optimizer = ipex.optimize(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    dtype=torch.float32,    # pure FP32\n",
    "    level=\"O1\",             # basic fusions\n",
    "    weights_prepack=False   # disable weight pre-packing\n",
    ")\n",
    "\n",
    "# Training loop (FP32 only)\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch:02d}\")\n",
    "\n",
    "    for xb, yb in pbar:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "        # Forward + loss (pure FP32)\n",
    "        loss = diffusion_loss(model, xb, yb)\n",
    "\n",
    "        # Backward + step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging\n",
    "        losses.append(loss.item())\n",
    "        mean_loss = np.mean(losses)\n",
    "        pbar.set_postfix(loss=f\"{mean_loss:.4f}\")\n",
    "\n",
    "        # if wandb.run:\n",
    "        #     wandb.log({\"train_loss\": loss.item()})\n",
    "\n",
    "    print(f\"Epoch {epoch}: mean loss {mean_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ad8ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 🟢 Cell 8 – Sampling (reverse diffusion)\n",
    "# -----------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def p_sample(model, x, t, y, eta=0.0):\n",
    "    \"\"\"One reverse step p(x_{t-1} | x_t) – DDIM.\"\"\"\n",
    "    eps_pred = model(x, t, y)\n",
    "    alpha_t = alphas[t][:, None, None, None]\n",
    "    alpha_bar = alphas_cumprod[t][:, None, None, None]\n",
    "    sqrt_inv_alpha = (1 / torch.sqrt(alpha_t))\n",
    "    x0_pred = sqrt_inv_alpha * (x - ((1 - alpha_t) / torch.sqrt(1 - alpha_bar)) * eps_pred)\n",
    "    if t[0] == 0:\n",
    "        return x0_pred\n",
    "    alpha_bar_prev = alphas_cumprod[t - 1][:, None, None, None]\n",
    "    sigma_t = eta * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar)) * torch.sqrt(1 - alpha_t / alpha_bar_prev)\n",
    "    noise = torch.randn_like(x) if eta > 0 else 0\n",
    "    x_prev = torch.sqrt(alpha_bar_prev) * x0_pred + torch.sqrt(1 - alpha_bar_prev - sigma_t**2) * eps_pred + sigma_t * noise\n",
    "    return x_prev\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_ddim(model, y, num_steps=T, eta=0.0):\n",
    "    \"\"\"Generate images given labels y (tensor shape [B]).\"\"\"\n",
    "    B = y.size(0)\n",
    "    x = torch.randn(B, *IMG_SHAPE, device=DEVICE)\n",
    "    for t_ in reversed(range(num_steps)):\n",
    "        t = torch.full((B,), t_, device=DEVICE, dtype=torch.long)\n",
    "        x = p_sample(model, x, t, y, eta)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b8f5b2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and xpu:0! (when checking argument for argument mat1 in method wrapper_XPU_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m labels = torch.arange(\u001b[32m10\u001b[39m, device=DEVICE)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# images = sample_ddim(model, labels, eta=0.0).cpu()  # shape [10,1,28,28]\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     images = \u001b[43msample_ddim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# still on XPU\u001b[39;00m\n\u001b[32m     10\u001b[39m images = images.cpu()  \u001b[38;5;66;03m# [10,1,28,28]\u001b[39;00m\n\u001b[32m     11\u001b[39m grid   = tvu.make_grid((images + \u001b[32m1\u001b[39m) / \u001b[32m2\u001b[39m, nrow=\u001b[32m10\u001b[39m, pad_value=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# un-normalize to [0,1]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\addod\\D7047E_exercise_group_5\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36msample_ddim\u001b[39m\u001b[34m(model, y, num_steps, eta)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(num_steps)):\n\u001b[32m     26\u001b[39m     t = torch.full((B,), t_, device=DEVICE, dtype=torch.long)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     x = \u001b[43mp_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\addod\\D7047E_exercise_group_5\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mp_sample\u001b[39m\u001b[34m(model, x, t, y, eta)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;129m@torch\u001b[39m.no_grad()\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mp_sample\u001b[39m(model, x, t, y, eta=\u001b[32m0.0\u001b[39m):\n\u001b[32m      6\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"One reverse step p(x_{t-1} | x_t) – DDIM.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     eps_pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     alpha_t = alphas[t][:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m      9\u001b[39m     alpha_bar = alphas_cumprod[t][:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\addod\\D7047E_exercise_group_5\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\addod\\D7047E_exercise_group_5\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mSimpleUNet.forward\u001b[39m\u001b[34m(self, x, t, y)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[33;03mx : noisy image  [B, 1, 28, 28]\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03mt : timestep     [B]\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[33;03my : digit label  [B]\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# --- combine timestep + label embeddings ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m cond_emb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtime_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m + \u001b[38;5;28mself\u001b[39m.label_emb(y)\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# --- encoder ---\u001b[39;00m\n\u001b[32m     78\u001b[39m d1 = \u001b[38;5;28mself\u001b[39m.down1(x, cond_emb)             \u001b[38;5;66;03m# 64 ch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\addod\\D7047E_exercise_group_5\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\addod\\D7047E_exercise_group_5\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\addod\\D7047E_exercise_group_5\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\addod\\D7047E_exercise_group_5\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\addod\\D7047E_exercise_group_5\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\addod\\D7047E_exercise_group_5\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cpu and xpu:0! (when checking argument for argument mat1 in method wrapper_XPU_addmm)"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 🟢 Cell 9 – Quick visual sanity check\n",
    "# -----------------------------------------------------------\n",
    "model.eval()\n",
    "labels = torch.arange(10, device=DEVICE)\n",
    "with torch.no_grad():\n",
    "    # images = sample_ddim(model, labels, eta=0.0).cpu()  # shape [10,1,28,28]\n",
    "    images = sample_ddim(model, labels, eta=0.0)  # still on XPU\n",
    "\n",
    "images = images.cpu()  # [10,1,28,28]\n",
    "grid   = tvu.make_grid((images + 1) / 2, nrow=10, pad_value=1)  # un-normalize to [0,1]\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.imshow(grid.permute(1,2,0), interpolation='nearest')\n",
    "plt.title(\"Diffusion-generated digits (0–9)\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "tvu.save_image(grid, \"diffusion_digits.png\")\n",
    "print(\"Saved grid → diffusion_digits.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2184bae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit ‘3’ — acc: 0.986  precision: 0.986  recall: 0.986  F1: 0.986\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 🟢 Cell 10 – Auxiliary classifier for quantitative scores\n",
    "# -----------------------------------------------------------\n",
    "class TinyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,3,padding=1),   nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "clf = TinyCNN().to(DEVICE)\n",
    "opt = torch.optim.Adam(clf.parameters(), lr=1e-3)\n",
    "\n",
    "# --- quick 2-epoch training on MNIST train set ---\n",
    "for epoch in range(2):\n",
    "    clf.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        logits = clf(xb)\n",
    "        loss   = F.cross_entropy(logits, yb)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def evaluate_generated(model, n_samples=1000, target_digit=3):\n",
    "    model.eval(); \n",
    "    clf.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:          # test_loader uses the same [-1,1] normalization\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = clf(xb)\n",
    "            all_preds.append(logits.argmax(1).cpu())\n",
    "            all_labels.append(yb.cpu())\n",
    "\n",
    "    all_preds  = torch.cat(all_preds).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    prec, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"micro\", zero_division=0)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return acc, prec, recall, f1\n",
    "\n",
    "acc, prec, rec, f1 = evaluate_generated(model, 1000, target_digit=3)\n",
    "print(f\"Digit ‘3’ — acc: {acc:.3f}  precision: {prec:.3f}  recall: {rec:.3f}  F1: {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0637422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to simple_unet_mnist_diffusion.pth\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 🟢 Cell 11 – Save the trained model\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# 1) Ensure model is in eval() mode (optional, but good practice)\n",
    "model.eval()\n",
    "\n",
    "# 2) Choose a path\n",
    "SAVE_PATH = \"simple_unet_mnist_diffusion.pth\"\n",
    "\n",
    "# 3) Save only the state_dict (recommended)\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "\n",
    "print(f\"Model weights saved to {SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48c81344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model reloaded and ready for inference.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 🟢 Cell 12 – Reload the model for future use\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# 1) Reconstruct the model architecture\n",
    "#    (must match exactly the class definition & init args!)\n",
    "loaded_model = SimpleUNet(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# 2) Load the saved weights\n",
    "#    map_location ensures compatibility if you switch device\n",
    "state_dict = torch.load(SAVE_PATH, map_location=DEVICE)\n",
    "loaded_model.load_state_dict(state_dict)\n",
    "\n",
    "# 3) Set to eval mode for inference\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"Model reloaded and ready for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45f33310",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "indices should be either on cpu or on the same device as the indexed tensor (cpu)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example: generate a “7”\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     sample = \u001b[43msample_ddim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m7\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                         \u001b[49m\u001b[43meta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Move to CPU, un-normalize & display\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\addod\\D7047E_exercise_group_5\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36msample_ddim\u001b[39m\u001b[34m(model, y, num_steps, eta)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(num_steps)):\n\u001b[32m     26\u001b[39m     t = torch.full((B,), t_, device=DEVICE, dtype=torch.long)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     x = \u001b[43mp_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\addod\\D7047E_exercise_group_5\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mp_sample\u001b[39m\u001b[34m(model, x, t, y, eta)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"One reverse step p(x_{t-1} | x_t) – DDIM.\"\"\"\u001b[39;00m\n\u001b[32m      7\u001b[39m eps_pred = model(x, t, y)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m alpha_t = \u001b[43malphas\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m      9\u001b[39m alpha_bar = alphas_cumprod[t][:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m     10\u001b[39m sqrt_inv_alpha = (\u001b[32m1\u001b[39m / torch.sqrt(alpha_t))\n",
      "\u001b[31mRuntimeError\u001b[39m: indices should be either on cpu or on the same device as the indexed tensor (cpu)"
     ]
    }
   ],
   "source": [
    "# Example: generate a “7”\n",
    "with torch.no_grad():\n",
    "    sample = sample_ddim(loaded_model,\n",
    "                         torch.tensor([7], device=DEVICE),\n",
    "                         eta=0.0)\n",
    "# Move to CPU, un-normalize & display\n",
    "import matplotlib.pyplot as plt\n",
    "img = ((sample + 1) / 2).cpu()[0,0]\n",
    "plt.imshow(img, cmap=\"gray\"); plt.axis(\"off\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7718d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier on real test images  →  acc=0.9862  F1=0.9861\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 🟢 Cell 13 – Final test on untouched data (classifier sanity-check)\n",
    "# -----------------------------------------------------------\n",
    "model.eval(); clf.eval()\n",
    "test_preds, test_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = clf(xb).cpu()\n",
    "        test_preds.append(logits.argmax(1))\n",
    "        test_labels.append(yb)\n",
    "test_preds  = torch.cat(test_preds).numpy()\n",
    "test_labels = torch.cat(test_labels).numpy()\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(test_labels, test_preds, average=\"macro\")\n",
    "acc = accuracy_score(test_labels, test_preds)\n",
    "print(f\"Classifier on real test images  →  acc={acc:.4f}  F1={f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
