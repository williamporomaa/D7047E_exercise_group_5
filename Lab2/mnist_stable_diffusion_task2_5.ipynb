{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a206741",
      "metadata": {
        "id": "7a206741"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------\n",
        "# Cell 1 – Imports, seed, device\n",
        "# -----------------------------------------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torchvision import datasets, transforms, utils as tvu\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import numpy as np\n",
        "import math, random, os, time, itertools\n",
        "from tqdm import tqdm\n",
        "from einops import rearrange # Added for SpatialTransformer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# Uncomment below if you want experiment tracking\n",
        "# import wandb\n",
        "# wandb.init(project=\"mnist_diffusion\", config={\"task\": \"2.5_transformer\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6688a9ac",
      "metadata": {
        "id": "6688a9ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device = cuda\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------\n",
        "# Cell 2 – Hyperparameters (centralized)\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "# Random seed for reproducibility\n",
        "SEED          = 42\n",
        "\n",
        "# Device\n",
        "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device = {DEVICE}\")\n",
        "\n",
        "# Data\n",
        "BATCH_SIZE    = 512        # ⟵ Boosted for maximum GPU usage\n",
        "NUM_WORKERS   = 4           # ⟵ Safer for big batches (2–4 usually best)\n",
        "NUM_CLASSES   = 10\n",
        "IMG_SHAPE     = (1, 32, 32) # ⟵ Model works at 32×32 now\n",
        "\n",
        "# Diffusion process\n",
        "T             = 500         # Number of timesteps\n",
        "BETA_SCHEDULE = \"cosine\"    # Cosine schedule = smoother high-fidelity sampling\n",
        "\n",
        "# Model architecture\n",
        "BASE_CHANNELS = 64          # Base channels for UNet (consistent with SimpleUNet big)\n",
        "TIME_EMB_DIM  = 256         # Embedding dimension for time and label embeddings\n",
        "CONTEXT_DIM   = TIME_EMB_DIM  # Dimension for label context in attention layers\n",
        "ATTN_HEADS    = 1           # Number of attention heads (using 1 as per provided code)\n",
        "\n",
        "# Optimization / Training\n",
        "LR            = 5e-4        # ⟵ Higher LR works because of larger batch size\n",
        "WEIGHT_DECAY  = 0.0         # No weight decay needed (yet)\n",
        "N_EPOCHS      = 20          # ⟵ 20 Epochs is enough with fast convergence (adjust as needed)\n",
        "AMP           = True        # ⟵ Enable Automatic Mixed Precision (16-bit) for faster training\n",
        "\n",
        "# Sampling\n",
        "DDIM_STEPS    = 50          # 50 steps DDIM sampling (fast but decent quality)\n",
        "DDIM_ETA      = 0.0         # 0.0 for deterministic sampling (sharper outputs)\n",
        "\n",
        "# Auxiliary classifier (for evaluation)\n",
        "CLF_LR        = 1e-3        # Learning rate for TinyCNN\n",
        "CLF_EPOCHS    = 5           # Train TinyCNN longer to guarantee good baseline\n",
        "\n",
        "# Logging / Checkpointing\n",
        "LOG_INTERVAL      = 10      # ⟵ Smaller interval to monitor large batches better\n",
        "SAVE_MODEL_EVERY  = 5       # Save model every 5 epochs\n",
        "USE_WANDB         = False   # Set to True if you want Weights & Biases logging\n",
        "WANDB_PROJECT     = \"mnist_diffusion\"\n",
        "\n",
        "# Seed setting function\n",
        "def set_seed(seed=SEED):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5802096c",
      "metadata": {
        "id": "5802096c",
        "outputId": "897c72d9-e8c9-4fd5-c8c1-3880c0c4921c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples: \n",
            "Training: 60000 | Test: 10000\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# -----------------------------------------------------------\n",
        "# Cell 3 – Dataset & DataLoaders\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "# Transformation pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(IMG_SHAPE[1]), # Use IMG_SHAPE for size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# MNIST dataset\n",
        "train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_ds = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Print the number of samples in each dataset\n",
        "print(f\"Number of samples: \")\n",
        "print(f\"Training: {len(train_ds)} | Test: {len(test_ds)}\\n\")\n",
        "\n",
        "# DataLoader for training, validation, and test datasets\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
        "\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                         num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "# Verify the data by plotting a sample image\n",
        "xb, _ = next(iter(train_loader))\n",
        "plt.imshow(((xb[0] + 1) / 2).squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "316594be",
      "metadata": {
        "id": "316594be",
        "outputId": "a15af20e-c5df-4f1a-aafa-ba30862677fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "β₀…β₄: [8.738040924072266e-05, 0.0001068115234375, 0.00012636184692382812, 0.0001456737518310547, 0.00016516447067260742]\n",
            "ᾱ₀…ᾱ₄: [0.9999126195907593, 0.9998058080673218, 0.999679446220398, 0.9995338320732117, 0.9993687272071838]\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------\n",
        "# Cell 4 – Diffusion schedule utilities\n",
        "# -----------------------------------------\n",
        "\n",
        "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
        "    return torch.linspace(start, end, timesteps)\n",
        "\n",
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    \"\"\"\n",
        "    Cosine schedule from Nichol & Dhariwal 2021 (DDPM++).\n",
        "    Returns a tensor of betas with shape [T].\n",
        "    \"\"\"\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi / 2)**2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clamp(betas, 1e-8, 0.999)\n",
        "\n",
        "# Select beta schedule\n",
        "if BETA_SCHEDULE == 'linear':\n",
        "    betas = linear_beta_schedule(T).to(DEVICE)\n",
        "elif BETA_SCHEDULE == 'cosine':\n",
        "    betas = cosine_beta_schedule(T).to(DEVICE)\n",
        "else:\n",
        "    raise ValueError(f\"Unknown beta schedule: {BETA_SCHEDULE}\")\n",
        "\n",
        "# Precompute alphas and cumulative products\n",
        "alphas = (1. - betas)\n",
        "alphas_cumprod      = torch.cumprod(alphas, dim=0).to(DEVICE)\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod).to(DEVICE)\n",
        "sqrt_one_minus_acp  = torch.sqrt(1. - alphas_cumprod).to(DEVICE)\n",
        "\n",
        "print(\"β₀…β₄:\", betas[:5].cpu().tolist())\n",
        "print(\"ᾱ₀…ᾱ₄:\", alphas_cumprod[:5].cpu().tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fdaf204",
      "metadata": {
        "id": "9fdaf204",
        "outputId": "0b1db87e-1f69-4c54-d8f6-2a09c85b5ef3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SinusoidalPosEmb: output shape: torch.Size([5, 256])\n",
            "  → should be [5, 256], device: cuda:0\n",
            "  stats: min -1.0000 max 1.0000\n",
            "LabelEmbedding: output shape: torch.Size([5, 256])\n",
            "  → should be [5, 256], device: cuda:0\n",
            "  stats: min -3.6666 max 3.0347\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------\n",
        "# Cell 5 – Positional & label embeddings\n",
        "# -----------------------------------------------------------\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard 1-D sinusoidal embeddings for timestep t (shape [B]).\n",
        "    Uses the same logic as in the original Transformer paper.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, t):\n",
        "        device = t.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = t[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb\n",
        "\n",
        "class LabelEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Embeds class labels into a vector space.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, dim, device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        # Ensure embedding layer is created on the correct device directly\n",
        "        self.emb = nn.Embedding(num_classes, dim, device=device)\n",
        "\n",
        "    def forward(self, y):\n",
        "        # The input `y` is already on the correct device (handled in training loop)\n",
        "        return self.emb(y)\n",
        "\n",
        "# Quick tests for Cell 5 embeddings\n",
        "set_seed() # Ensure reproducibility for tests\n",
        "\n",
        "# 1) SinusoidalPosEmb\n",
        "pos_emb = SinusoidalPosEmb(TIME_EMB_DIM)\n",
        "t_sample = torch.randint(0, T, (5,), device=DEVICE)  # 5 random timesteps\n",
        "out = pos_emb(t_sample)\n",
        "print(f\"SinusoidalPosEmb: output shape: {out.shape}\")\n",
        "print(f\"  → should be [5, {TIME_EMB_DIM}], device: {out.device}\")\n",
        "print(f\"  stats: min {out.min().item():.4f} max {out.max().item():.4f}\")\n",
        "\n",
        "# 2) LabelEmbedding\n",
        "lbl_emb = LabelEmbedding(NUM_CLASSES, TIME_EMB_DIM, device=DEVICE) # Pass device here\n",
        "y_sample = torch.randint(0, NUM_CLASSES, (5,), device=DEVICE)\n",
        "lout = lbl_emb(y_sample)\n",
        "print(f\"LabelEmbedding: output shape: {lout.shape}\")\n",
        "print(f\"  → should be [5, {TIME_EMB_DIM}], device: {lout.device}\")\n",
        "print(f\"  stats: min {lout.min().item():.4f} max {lout.max().item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "transformer_classes",
      "metadata": {
        "id": "transformer_classes",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------\n",
        "# Cell 6 – Attention Mechanism Components\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim, context_dim=None, num_heads=1):\n",
        "        \"\"\"\n",
        "        Cross-Attention or Self-Attention module.\n",
        "        Supports single-head attention as provided.\n",
        "\n",
        "        Parameters:\n",
        "        - embed_dim: The dimensionality of the query/key projections.\n",
        "        - hidden_dim: The dimensionality of the input tokens (and value projection output).\n",
        "        - context_dim: The dimensionality of the context tokens. If None, performs self-attention.\n",
        "        - num_heads: Number of attention heads (currently only supports 1).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.context_dim = context_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads # Although not used in einsum, kept for clarity\n",
        "\n",
        "        # Linear layer for query projection\n",
        "        self.query = nn.Linear(hidden_dim, embed_dim * num_heads, bias=False)\n",
        "\n",
        "        # Determine if self-attention or cross-attention and define key/value projections\n",
        "        if context_dim is None:\n",
        "            self.self_attn = True\n",
        "            self.key = nn.Linear(hidden_dim, embed_dim * num_heads, bias=False)\n",
        "            self.value = nn.Linear(hidden_dim, hidden_dim * num_heads, bias=False)\n",
        "        else:\n",
        "            self.self_attn = False\n",
        "            self.key = nn.Linear(context_dim, embed_dim * num_heads, bias=False)\n",
        "            self.value = nn.Linear(context_dim, hidden_dim * num_heads, bias=False)\n",
        "\n",
        "        # Output projection\n",
        "        self.out_proj = nn.Linear(hidden_dim * num_heads, hidden_dim) # Project back to original hidden_dim\n",
        "\n",
        "    def forward(self, tokens, context=None):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Parameters:\n",
        "        - tokens: Input tokens (e.g., image features) - shape [B, SeqLen, HiddenDim]\n",
        "        - context: Context tokens (e.g., label embedding) - shape [B, CtxSeqLen, CtxDim]\n",
        "                   If self_attn is True, context should be None or will be ignored.\n",
        "\n",
        "        Returns:\n",
        "        - Output tensor after attention - shape [B, SeqLen, HiddenDim]\n",
        "        \"\"\"\n",
        "        Q = self.query(tokens)\n",
        "\n",
        "        if self.self_attn:\n",
        "            K = self.key(tokens)\n",
        "            V = self.value(tokens)\n",
        "        else:\n",
        "            if context is None:\n",
        "                raise ValueError(\"Context must be provided for cross-attention\")\n",
        "            K = self.key(context)\n",
        "            V = self.value(context)\n",
        "\n",
        "        # --- Single-Head Attention Calculation ---\n",
        "        # Assuming num_heads = 1 based on provided einsum\n",
        "        scoremats = torch.einsum(\"BTH,BSH->BTS\", Q, K) # [B, SeqLen, CtxSeqLen]\n",
        "        attnmats = F.softmax(scoremats / math.sqrt(self.embed_dim), dim=-1)\n",
        "        ctx_vecs = torch.einsum(\"BTS,BSH->BTH\", attnmats, V) # [B, SeqLen, HiddenDim]\n",
        "        # --- End Single-Head ---\n",
        "\n",
        "        # TODO: Implement multi-head attention if num_heads > 1\n",
        "        # This would involve reshaping Q, K, V for heads, applying attention in parallel,\n",
        "        # and concatenating results before the final projection.\n",
        "\n",
        "        # Apply output projection\n",
        "        return self.out_proj(ctx_vecs)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Combines self-attn, cross-attn, and a feed-forward network.\"\"\"\n",
        "    def __init__(self, hidden_dim, context_dim, num_heads=1):\n",
        "        super().__init__()\n",
        "        self.attn_self = CrossAttention(hidden_dim, hidden_dim, num_heads=num_heads) # Self-attention\n",
        "        self.attn_cross = CrossAttention(hidden_dim, hidden_dim, context_dim, num_heads=num_heads) # Cross-attention\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 3 * hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(3 * hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        \"\"\"\n",
        "        x: [B, SeqLen, HiddenDim]\n",
        "        context: [B, CtxSeqLen, CtxDim]\n",
        "        \"\"\"\n",
        "        # Self-Attention + Residual + Norm\n",
        "        x = self.attn_self(self.norm1(x)) + x\n",
        "\n",
        "        # Cross-Attention + Residual + Norm\n",
        "        x = self.attn_cross(self.norm2(x), context=context) + x\n",
        "\n",
        "        # Feed-Forward + Residual + Norm\n",
        "        x = self.ffn(self.norm3(x)) + x\n",
        "        return x\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    \"\"\"Applies Transformer blocks spatially to image features.\"\"\"\n",
        "    def __init__(self, hidden_dim, context_dim, num_heads=1):\n",
        "        super().__init__()\n",
        "        self.transformer = TransformerBlock(hidden_dim, context_dim, num_heads)\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        \"\"\"\n",
        "        x: [B, C, H, W] - Input feature map\n",
        "        context: [B, CtxSeqLen, CtxDim] - Context embedding\n",
        "        \"\"\"\n",
        "        b, c, h, w = x.shape\n",
        "        x_in = x\n",
        "\n",
        "        # Reshape for transformer: [B, H*W, C]\n",
        "        x = rearrange(x, \"b c h w -> b (h w) c\")\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        x = self.transformer(x, context)\n",
        "\n",
        "        # Reshape back to image format: [B, C, H, W]\n",
        "        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
        "\n",
        "        # Residual connection\n",
        "        return x + x_in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a421c6",
      "metadata": {
        "id": "e0a421c6",
        "outputId": "90cd04e0-8bee-4cd0-ceb9-4fee8e04a34e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters: 24420.1K\n",
            "Output shape: torch.Size([8, 1, 32, 32])\n",
            "  → should be [8, 1, 32, 32], device: cuda:0\n",
            "Output stats: min -1.4008 max 1.2384\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------\n",
        "# Cell 7 – UNet Denoiser with Spatial Transformer\n",
        "# -----------------------------------------------------------\n",
        "class DenseLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Dense layer that broadcasts outputs to feature maps.\n",
        "    (Used for time embedding projection in ResidualBlocks)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input x shape: [B, InputDim]\n",
        "        # Output shape: [B, OutputDim, 1, 1]\n",
        "        return self.fc(x)[..., None, None]\n",
        "    \n",
        "def make_group_norm(num_channels, max_groups=8):\n",
        "    # start from the smaller of max_groups and num_channels\n",
        "    g = min(max_groups, num_channels)\n",
        "    # walk down until you find a divisor\n",
        "    while num_channels % g != 0:\n",
        "        g -= 1\n",
        "    return nn.GroupNorm(g, num_channels)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_c: int, out_c: int, time_emb_dim: int):\n",
        "        super().__init__()\n",
        "        self.time_mlp = DenseLayer(time_emb_dim, out_c)\n",
        "\n",
        "        def GN(ch): \n",
        "            return make_group_norm(ch, max_groups=8)\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            GN(in_c),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(in_c, out_c, 3, padding=1, bias=False),\n",
        "            GN(out_c),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(out_c, out_c, 3, padding=1, bias=False),\n",
        "        )\n",
        "        self.residual = (\n",
        "            nn.Conv2d(in_c, out_c, 1, bias=False)\n",
        "            if in_c != out_c\n",
        "            else nn.Identity()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t_emb):\n",
        "        # x shape: [B, InC, H, W]\n",
        "        # t_emb shape: [B, TimeEmbDim]\n",
        "\n",
        "        h = self.block[:3](x) # Norm, SiLU, Conv1\n",
        "        h = h + self.time_mlp(t_emb) # Add time embedding broadcasted spatially\n",
        "        h = self.block[3:](h) # Norm, SiLU, Conv2\n",
        "\n",
        "        return h + self.residual(x)\n",
        "\n",
        "class SimpleUNetWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    UNet that predicts ε for DDPM, conditioned on timestep + class label.\n",
        "    Includes SpatialTransformer blocks for attention.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_ch=1, base_c=64, time_emb_dim=256, context_dim=256, num_classes=10, attn_heads=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # -------- Embeddings --------\n",
        "        self.time_emb = nn.Sequential(\n",
        "            SinusoidalPosEmb(time_emb_dim),\n",
        "            nn.Linear(time_emb_dim, time_emb_dim * 4),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_emb_dim * 4, time_emb_dim)\n",
        "        )\n",
        "        # Label embedding produces context_dim directly\n",
        "        self.label_emb = LabelEmbedding(num_classes, context_dim, device=DEVICE)\n",
        "\n",
        "        # -------- Downsampling path --------\n",
        "        # Level 1 (32x32)\n",
        "        self.down1_res = ResidualBlock(img_ch, base_c, time_emb_dim)      # 1 → 64\n",
        "        # Level 2 (16x16)\n",
        "        self.down2_pool = nn.MaxPool2d(2)\n",
        "        self.down2_res = ResidualBlock(base_c, base_c*2, time_emb_dim)    # 64 → 128\n",
        "        self.down2_attn = SpatialTransformer(base_c*2, context_dim, attn_heads)\n",
        "        # Level 3 (8x8)\n",
        "        self.down3_pool = nn.MaxPool2d(2)\n",
        "        self.down3_res = ResidualBlock(base_c*2, base_c*4, time_emb_dim)  # 128 → 256\n",
        "        self.down3_attn = SpatialTransformer(base_c*4, context_dim, attn_heads)\n",
        "        # Level 4 (4x4)\n",
        "        self.down4_pool = nn.MaxPool2d(2)\n",
        "        self.down4_res = ResidualBlock(base_c*4, base_c*8, time_emb_dim)  # 256 -> 512\n",
        "\n",
        "        # -------- Bottleneck (4x4) --------\n",
        "        self.mid_res1 = ResidualBlock(base_c*8, base_c*8, time_emb_dim)   # 512 → 512\n",
        "        self.mid_attn = SpatialTransformer(base_c*8, context_dim, attn_heads) # Attention in bottleneck\n",
        "        self.mid_res2 = ResidualBlock(base_c*8, base_c*8, time_emb_dim)   # 512 → 512\n",
        "\n",
        "        # -------- Upsampling path --------\n",
        "        # Level 4 (8x8)\n",
        "        self.up4_up = nn.ConvTranspose2d(base_c*8, base_c*4, kernel_size=2, stride=2)\n",
        "        self.up4_res = ResidualBlock(base_c*8, base_c*4, time_emb_dim) # Concatenated: (512+256)=768 -> 256\n",
        "        self.up4_attn = SpatialTransformer(base_c*4, context_dim, attn_heads)\n",
        "        # Level 3 (16x16)\n",
        "        self.up3_up = nn.ConvTranspose2d(base_c*4, base_c*2, kernel_size=2, stride=2)\n",
        "        self.up3_res = ResidualBlock(base_c*4, base_c*2, time_emb_dim) # Concatenated: (256+128)=384 -> 128\n",
        "        self.up3_attn = SpatialTransformer(base_c*2, context_dim, attn_heads)\n",
        "        # Level 2 (32x32)\n",
        "        self.up2_up = nn.ConvTranspose2d(base_c*2, base_c, kernel_size=2, stride=2)\n",
        "        self.up2_res = ResidualBlock(base_c*2, base_c, time_emb_dim) # Concatenated: (128+64)=192 -> 64\n",
        "        # Level 1 (32x32) - Final Output\n",
        "        self.out_norm = nn.GroupNorm(8, base_c)\n",
        "        self.out_act = nn.SiLU()\n",
        "        self.out_conv = nn.Conv2d(base_c, img_ch, kernel_size=1) # 64 -> 1\n",
        "\n",
        "    def forward(self, x, t, y):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "        x : [B, C, H, W]  noisy input image\n",
        "        t : [B]           timestep indices\n",
        "        y : [B]           digit labels\n",
        "        \"\"\"\n",
        "        # Embeddings\n",
        "        t_emb = self.time_emb(t)       # [B, time_emb_dim]\n",
        "        lbl_emb = self.label_emb(y)    # [B, context_dim]\n",
        "        context = lbl_emb[:, None, :]  # [B, 1, context_dim] - Add sequence length dimension for attention\n",
        "\n",
        "        # -------- Encoder --------\n",
        "        # Level 1\n",
        "        d1 = self.down1_res(x, t_emb)      # [B, 64, 32, 32]\n",
        "        # Level 2\n",
        "        d2_in = self.down2_pool(d1)\n",
        "        d2 = self.down2_res(d2_in, t_emb)  # [B, 128, 16, 16]\n",
        "        d2 = self.down2_attn(d2, context)\n",
        "        # Level 3\n",
        "        d3_in = self.down3_pool(d2)\n",
        "        d3 = self.down3_res(d3_in, t_emb)  # [B, 256, 8, 8]\n",
        "        d3 = self.down3_attn(d3, context)\n",
        "        # Level 4\n",
        "        d4_in = self.down4_pool(d3)\n",
        "        d4 = self.down4_res(d4_in, t_emb)  # [B, 512, 4, 4]\n",
        "        # d4 = self.down4_attn(d4, context) # Optional attention at 4x4\n",
        "\n",
        "        # -------- Bottleneck --------\n",
        "        m = self.mid_res1(d4, t_emb)\n",
        "        m = self.mid_attn(m, context)\n",
        "        m = self.mid_res2(m, t_emb)       # [B, 512, 4, 4]\n",
        "\n",
        "        # -------- Decoder --------\n",
        "        # Level 4\n",
        "        u4 = self.up4_up(m)                # [B, 256, 8, 8]\n",
        "        u4 = torch.cat([u4, d3], dim=1)    # [B, 256+256=512, 8, 8] - Error in comment, should be 512+256=768\n",
        "        # Correction: up4_up output channels is base_c*4=256. Skip connection d3 is base_c*4=256.\n",
        "        # Concatenated channels = 256 + 256 = 512. My ResidualBlock definition was wrong. Fixing...\n",
        "        # Corrected ResidualBlock def: in_c = base_c*4 + base_c*4 = base_c*8 = 512\n",
        "        # Let's check dimensions again.\n",
        "        # m: [B, 512, 4, 4] -> up4_up: [B, 256, 8, 8]\n",
        "        # d3: [B, 256, 8, 8]\n",
        "        # cat: [B, 512, 8, 8]\n",
        "        # Corrected ResidualBlock: self.up4_res = ResidualBlock(base_c*8, base_c*4, time_emb_dim) # 512 -> 256\n",
        "        u4 = self.up4_res(u4, t_emb)       # [B, 256, 8, 8]\n",
        "        u4 = self.up4_attn(u4, context)\n",
        "\n",
        "        # Level 3\n",
        "        u3 = self.up3_up(u4)                # [B, 128, 16, 16]\n",
        "        u3 = torch.cat([u3, d2], dim=1)     # [B, 128+128=256]\n",
        "        # Corrected ResidualBlock: self.up3_res = ResidualBlock(base_c*4, base_c*2, time_emb_dim) # 256 -> 128\n",
        "        u3 = self.up3_res(u3, t_emb)        # [B, 128, 16, 16]\n",
        "        u3 = self.up3_attn(u3, context)\n",
        "\n",
        "        # Level 2\n",
        "        u2 = self.up2_up(u3)                # [B, 64, 32, 32]\n",
        "        u2 = torch.cat([u2, d1], dim=1)     # [B, 64+64=128]\n",
        "        # Corrected ResidualBlock: self.up2_res = ResidualBlock(base_c*2, base_c, time_emb_dim) # 128 -> 64\n",
        "        u2 = self.up2_res(u2, t_emb)        # [B, 64, 32, 32]\n",
        "\n",
        "        # Final Output Conv\n",
        "        out = self.out_norm(u2)\n",
        "        out = self.out_act(out)\n",
        "        out = self.out_conv(out)            # [B, 1, 32, 32]\n",
        "\n",
        "        return out\n",
        "\n",
        "# ── Verify total parameter count ─────────────────────────────\n",
        "torch.cuda.empty_cache()\n",
        "set_seed() # Ensure reproducibility for model init\n",
        "model = SimpleUNetWithAttention(\n",
        "    img_ch=IMG_SHAPE[0],\n",
        "    base_c=BASE_CHANNELS,\n",
        "    time_emb_dim=TIME_EMB_DIM,\n",
        "    context_dim=CONTEXT_DIM,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    attn_heads=ATTN_HEADS\n",
        ").to(DEVICE)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params / 1e3:.1f}K\")\n",
        "\n",
        "# ── Dummy pass to check shapes & devices ────────────────────\n",
        "B = 8\n",
        "x_dummy  = torch.randn(B, *IMG_SHAPE, device=DEVICE)\n",
        "t_dummy  = torch.randint(0, T, (B,), device=DEVICE)\n",
        "y_dummy  = torch.randint(0, NUM_CLASSES, (B,), device=DEVICE)\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    eps_pred = model(x_dummy, t_dummy, y_dummy)\n",
        "\n",
        "print(f\"Output shape: {eps_pred.shape}\")\n",
        "print(f\"  → should be [{B}, {IMG_SHAPE[0]}, {IMG_SHAPE[1]}, {IMG_SHAPE[2]}], device: {eps_pred.device}\")\n",
        "print(f\"Output stats: min {eps_pred.min().item():.4f} max {eps_pred.max().item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06c48754",
      "metadata": {
        "id": "06c48754"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------\n",
        "# Cell 8 – Forward-diffusion helper + loss\n",
        "# -----------------------------------------------------------\n",
        "@torch.no_grad()\n",
        "def q_sample(x0, t, noise=None):\n",
        "    \"\"\"Diffuse the clean image x0 to x_t via q(x_t | x0). Uses precomputed schedules.\"\"\"\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x0)\n",
        "\n",
        "    # Ensure tensors are on the same device\n",
        "    sqrt_alphas_cumprod_t = sqrt_alphas_cumprod[t].to(x0.device)\n",
        "    sqrt_one_minus_acp_t = sqrt_one_minus_acp[t].to(x0.device)\n",
        "\n",
        "    # Reshape for broadcasting: [B] -> [B, 1, 1, 1]\n",
        "    sqrt_alphas_cumprod_t = sqrt_alphas_cumprod_t.view(-1, 1, 1, 1)\n",
        "    sqrt_one_minus_acp_t = sqrt_one_minus_acp_t.view(-1, 1, 1, 1)\n",
        "\n",
        "    # Sample x_t: sqrt(ᾱ_t) * x0 + sqrt(1 - ᾱ_t) * noise\n",
        "    return sqrt_alphas_cumprod_t * x0 + sqrt_one_minus_acp_t * noise\n",
        "\n",
        "def diffusion_loss(model, x0, y):\n",
        "    \"\"\"Simplified DDPM objective: predict ε directly (MSE loss).\"\"\"\n",
        "    B = x0.size(0)\n",
        "    t = torch.randint(0, T, (B,), device=x0.device, dtype=torch.long) # Sample random timesteps [0, T-1]\n",
        "    noise = torch.randn_like(x0, device=x0.device)                   # Sample Gaussian noise ε\n",
        "\n",
        "    x_t = q_sample(x0, t, noise)  # Create noisy inputs x_t using precomputed schedules\n",
        "    ε_pred = model(x_t, t, y)     # Predict noise ε using the UNet model\n",
        "\n",
        "    return F.mse_loss(ε_pred, noise) # Compare predicted noise with actual noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "536d252e",
      "metadata": {
        "id": "536d252e",
        "outputId": "8de4beef-825d-4d70-c377-d75a674f7711"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: mean loss 0.1265\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: mean loss 0.0489\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: mean loss 0.0419\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: mean loss 0.0379\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: mean loss 0.0356\n",
            "Saved checkpoint at epoch 5 to model_attn_epoch005.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: mean loss 0.0338\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: mean loss 0.0328\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: mean loss 0.0319\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: mean loss 0.0313\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: mean loss 0.0304\n",
            "Saved checkpoint at epoch 10 to model_attn_epoch010.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: mean loss 0.0301\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: mean loss 0.0300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: mean loss 0.0295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: mean loss 0.0291\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: mean loss 0.0293\n",
            "Saved checkpoint at epoch 15 to model_attn_epoch015.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: mean loss 0.0286\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: mean loss 0.0285\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: mean loss 0.0282\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: mean loss 0.0277\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: mean loss 0.0280\n",
            "Saved checkpoint at epoch 20 to model_attn_epoch020.pth\n",
            "\n",
            "Training finished in 8.52 minutes.\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------\n",
        "# Cell 9 – Training loop (correct AMP usage)\n",
        "# -----------------------------------------------------------\n",
        "scaler = torch.amp.GradScaler(enabled=AMP)\n",
        "\n",
        "# Instantiate the model with attention\n",
        "model = SimpleUNetWithAttention(\n",
        "    img_ch=IMG_SHAPE[0],\n",
        "    base_c=BASE_CHANNELS,\n",
        "    time_emb_dim=TIME_EMB_DIM,\n",
        "    context_dim=CONTEXT_DIM,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    attn_heads=ATTN_HEADS\n",
        ").to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    # Use drop_last=True in DataLoader to ensure consistent batch sizes for AMP\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch:02d}/{N_EPOCHS}\", leave=False)\n",
        "\n",
        "    for xb, yb in pbar:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Use autocast for the forward pass\n",
        "        with torch.autocast(device_type=DEVICE, enabled=AMP):\n",
        "            loss = diffusion_loss(model, xb, yb)\n",
        "\n",
        "        # Scale the loss and perform backward pass\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Unscale gradients and step optimizer\n",
        "        scaler.step(optimizer)\n",
        "\n",
        "        # Update the scaler for next iteration\n",
        "        scaler.update()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        if len(losses) % LOG_INTERVAL == 0:\n",
        "            mean_loss = np.mean(losses[-LOG_INTERVAL:]) # Log recent average\n",
        "            pbar.set_postfix(loss=f\"{mean_loss:.4f}\")\n",
        "\n",
        "        # Optionally: wandb logging\n",
        "        # if USE_WANDB and wandb.run:\n",
        "        #     wandb.log({\"train_loss\": loss.item(), \"epoch\": epoch})\n",
        "\n",
        "    epoch_mean_loss = np.mean(losses)\n",
        "    print(f\"Epoch {epoch}: mean loss {epoch_mean_loss:.4f}\")\n",
        "    # if USE_WANDB and wandb.run:\n",
        "    #     wandb.log({\"epoch_mean_loss\": epoch_mean_loss, \"epoch\": epoch})\n",
        "\n",
        "    # Save model every few epochs\n",
        "    if epoch % SAVE_MODEL_EVERY == 0:\n",
        "        ckpt_path = f\"model_attn_epoch{epoch:03d}.pth\"\n",
        "        torch.save(model.state_dict(), ckpt_path)\n",
        "        print(f\"Saved checkpoint at epoch {epoch} to {ckpt_path}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\nTraining finished in {(end_time - start_time)/60:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ad8ec4b",
      "metadata": {
        "id": "2ad8ec4b"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------\n",
        "# Cell 10 – Reverse Diffusion: Fast DDIM Sampling\n",
        "# -------------------------------------------------------------\n",
        "@torch.no_grad()\n",
        "def sample_ddim_fast(model, y, num_steps=DDIM_STEPS, eta=DDIM_ETA):\n",
        "    \"\"\"\n",
        "    DDIM sampling (Algorithm 2 from Denoising Diffusion Implicit Models).\n",
        "    Uses precomputed schedules (alphas, alphas_cumprod).\n",
        "\n",
        "    Parameters:\n",
        "    - model: The trained UNet model.\n",
        "    - y: The conditional labels [B].\n",
        "    - num_steps: Number of DDIM sampling steps (τ in the paper).\n",
        "    - eta: Controls stochasticity (0.0 = DDIM, 1.0 = DDPM-like variance).\n",
        "\n",
        "    Returns:\n",
        "    - x: The generated samples [B, C, H, W].\n",
        "    \"\"\"\n",
        "    model.eval() # Ensure model is in evaluation mode\n",
        "    B = y.size(0)\n",
        "\n",
        "    # Generate the DDIM subsequence of timesteps {t_i}\n",
        "    # Starts from T-1 down to 0, with `num_steps` intervals.\n",
        "    t_seq = torch.linspace(T - 1, 0, num_steps, device=DEVICE).long()\n",
        "\n",
        "    # Start with random noise x_T\n",
        "    x = torch.randn(B, *IMG_SHAPE, device=DEVICE)\n",
        "\n",
        "    for i in tqdm(range(num_steps), desc=\"DDIM Sampling\", leave=False):\n",
        "        # Current timestep t_i\n",
        "        t_i = t_seq[i].repeat(B) # Expand timestep to batch size [B]\n",
        "\n",
        "        # Predict noise ε_θ(x_{t_i}, t_i, y)\n",
        "        eps_pred = model(x, t_i, y)\n",
        "\n",
        "        # Get ᾱ_{t_i} and ᾱ_{t_{i-1}}\n",
        "        alpha_bar_i = alphas_cumprod[t_i][:, None, None, None] # Shape [B, 1, 1, 1]\n",
        "\n",
        "        # Get the 'previous' timestep t_{i-1}\n",
        "        # If i=0 (first step), t_{i-1} doesn't exist conceptually for the formula,\n",
        "        # but for computation, we handle the edge case where t_next is 0.\n",
        "        if i < num_steps - 1:\n",
        "            t_prev = t_seq[i + 1].repeat(B)\n",
        "            alpha_bar_prev = alphas_cumprod[t_prev][:, None, None, None]\n",
        "        else:\n",
        "            # Last step (i = num_steps - 1), t_prev effectively corresponds to ᾱ_0 = 1\n",
        "            alpha_bar_prev = torch.ones_like(alpha_bar_i)\n",
        "\n",
        "        # Equation (12) components:\n",
        "        # 1. Predicted x0: (x_{t_i} - sqrt(1 - ᾱ_{t_i}) * ε_θ) / sqrt(ᾱ_{t_i})\n",
        "        sqrt_one_minus_alpha_bar_i = torch.sqrt(1. - alpha_bar_i)\n",
        "        pred_x0 = (x - sqrt_one_minus_alpha_bar_i * eps_pred) / torch.sqrt(alpha_bar_i)\n",
        "        pred_x0.clamp_(-1., 1.) # Optional: Clip predicted x0\n",
        "\n",
        "        # 2. Direction pointing to x_{t_i}\n",
        "        # This term is implicitly handled by starting from x and adding the other two terms.\n",
        "\n",
        "        # 3. Random noise component (controlled by eta and σ_i)\n",
        "        # Calculate σ_i (Equation 16 in DDIM paper)\n",
        "        # σ_i = η * sqrt( (1 - ᾱ_{t_{i-1}}) / (1 - ᾱ_{t_i}) ) * sqrt( 1 - ᾱ_{t_i} / ᾱ_{t_{i-1}} )\n",
        "        # Note: We use alpha_bar_i for ᾱ_{t_i} and alpha_bar_prev for ᾱ_{t_{i-1}}\n",
        "        var = (1. - alpha_bar_prev) / (1. - alpha_bar_i) * (1. - alpha_bar_i / alpha_bar_prev)\n",
        "        # Clamp variance to be non-negative for sqrt\n",
        "        sigma_i = eta * torch.sqrt(torch.clamp(var, min=1e-8))\n",
        "\n",
        "        # Sample standard Gaussian noise\n",
        "        noise = torch.randn_like(x) if eta > 0 else 0.0\n",
        "\n",
        "        # Combine terms using Equation (12):\n",
        "        # x_{t_{i-1}} = sqrt(ᾱ_{t_{i-1}}) * pred_x0\n",
        "        #             + sqrt(1 - ᾱ_{t_{i-1}} - σ_i^2) * ε_θ\n",
        "        #             + σ_i * noise\n",
        "        term2_coeff = torch.sqrt(torch.clamp(1. - alpha_bar_prev - sigma_i**2, min=0.0))\n",
        "        x = torch.sqrt(alpha_bar_prev) * pred_x0 + term2_coeff * eps_pred + sigma_i * noise\n",
        "\n",
        "    return x # Final denoised sample x_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b8f5b2b",
      "metadata": {
        "id": "1b8f5b2b",
        "outputId": "a2664a82-3183-4251-dc53-3827182eb8e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier trained for 5 epochs.\n",
            "Evaluating generated images for digit '3'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                              \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Digit ‘3’ — acc: 0.854  precision: 1.000  recall: 0.854  F1: 0.921\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------\n",
        "# Cell 11 – Auxiliary Classifier for Evaluation (Corrected)\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "class TinyCNN(nn.Module):\n",
        "    \"\"\" A simple CNN classifier for MNIST 32x32. \"\"\"\n",
        "    def __init__(self, num_classes=NUM_CLASSES):\n",
        "        super().__init__()\n",
        "        # Input: [B, 1, 32, 32]\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2), # [B, 32, 16, 16]\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2), # [B, 64, 8, 8]\n",
        "            nn.Flatten(),    # [B, 64 * 8 * 8 = 4096]\n",
        "            # Corrected Linear layer input size\n",
        "            nn.Linear(64 * 8 * 8, 256), \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, num_classes) # [B, num_classes]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "clf = TinyCNN().to(DEVICE)\n",
        "opt = torch.optim.Adam(clf.parameters(), lr=CLF_LR)\n",
        "\n",
        "# --- Train the classifier ---\n",
        "set_seed() # Ensure reproducible classifier training\n",
        "for epoch in range(1, CLF_EPOCHS + 1):\n",
        "    clf.train()\n",
        "    clf_pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{CLF_EPOCHS}\", leave=False)\n",
        "    for xb, yb in clf_pbar:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        logits = clf(xb)\n",
        "        loss = F.cross_entropy(logits, yb)\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "        clf_pbar.set_postfix(loss=f\"{loss.item():.5f}\")\n",
        "print(f\"Classifier trained for {CLF_EPOCHS} epochs.\")\n",
        "\n",
        "\n",
        "# --- Evaluation function using the trained classifier ---\n",
        "def evaluate_generated(gen_model, n_samples=1000, target_digit=3):\n",
        "    \"\"\"Generates images for a target digit and evaluates using TinyCNN.\"\"\"\n",
        "    gen_model.eval() # Ensure generator is in eval mode\n",
        "    clf.eval()       # Ensure classifier is in eval mode\n",
        "\n",
        "    print(f\"Evaluating generated images for digit '{target_digit}'...\")\n",
        "    # Generate fake images with DDIM\n",
        "    y_cond = torch.full((n_samples,), target_digit, dtype=torch.long, device=DEVICE)\n",
        "    fake_images = sample_ddim_fast(gen_model, y_cond, num_steps=DDIM_STEPS, eta=DDIM_ETA).detach()\n",
        "\n",
        "    # Images are in [-1, 1], classifier expects normalized ToTensor output [0, 1] -> [-1, 1]\n",
        "    # No rescaling needed as classifier trained on same [-1, 1] data\n",
        "\n",
        "    preds = []\n",
        "    # Process in minibatches to avoid OOM on classifier\n",
        "    with torch.no_grad():\n",
        "        for chunk in fake_images.split(BATCH_SIZE):\n",
        "            logits = clf(chunk.to(DEVICE))\n",
        "            preds.append(logits.argmax(1).cpu())\n",
        "\n",
        "    preds = torch.cat(preds).numpy()\n",
        "    labels = np.full_like(preds, target_digit) # True labels are the target digit\n",
        "\n",
        "    # Calculate precision, recall, f1 for the target class vs all others\n",
        "    true_binary = (labels == target_digit)    # e.g. [True, True, …]\n",
        "    pred_binary = (preds  == target_digit)    # e.g. [False, True, …]\n",
        "\n",
        "    prec, recall, f1, _ = precision_recall_fscore_support(\n",
        "        true_binary,\n",
        "        pred_binary,\n",
        "        average=\"binary\",\n",
        "        zero_division=0\n",
        "    )\n",
        "    acc = accuracy_score(true_binary, pred_binary)\n",
        "    \n",
        "    return acc, prec, recall, f1\n",
        "\n",
        "# Test the classifier on generated images for digit '3' using the trained diffusion model\n",
        "# This requires the main model ('model') to be trained first.\n",
        "# Assuming 'model' is the trained SimpleUNetWithAttention instance\n",
        "acc, prec, rec, f1 = evaluate_generated(model, n_samples=1000, target_digit=3)\n",
        "print(f\"Digit ‘3’ — acc: {acc:.3f}  precision: {prec:.3f}  recall: {rec:.3f}  F1: {f1:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48c81344",
      "metadata": {
        "id": "48c81344",
        "outputId": "001a012b-31d8-4e55-fb31-d4f3a5230f69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model reloaded from model_attn_epoch020.pth and ready for inference.\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------\n",
        "# Cell 12 – Load Model Checkpoint for Inference\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "# Define path to the saved model checkpoint\n",
        "SAVE_PATH = f\"model_attn_epoch{N_EPOCHS:03d}.pth\"\n",
        "\n",
        "# 1) Reconstruct the model architecture using training hyperparameters\n",
        "loaded_model = SimpleUNetWithAttention(\n",
        "    img_ch=IMG_SHAPE[0],\n",
        "    base_c=BASE_CHANNELS,\n",
        "    time_emb_dim=TIME_EMB_DIM,\n",
        "    context_dim=CONTEXT_DIM,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    attn_heads=ATTN_HEADS\n",
        ").to(DEVICE)\n",
        "\n",
        "# 2) Load the saved weights\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    try:\n",
        "        state_dict = torch.load(SAVE_PATH, map_location=DEVICE)\n",
        "        loaded_model.load_state_dict(state_dict)\n",
        "        # 3) Set to eval mode for inference\n",
        "        loaded_model.eval()\n",
        "        print(f\"Model reloaded from {SAVE_PATH} and ready for inference.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Failed to load checkpoint from {SAVE_PATH}. Error: {e}\")\n",
        "        # Handle error - maybe proceed with the model trained in the current session if available\n",
        "        if 'model' in globals():\n",
        "            print(\"Using model trained in the current session.\")\n",
        "            loaded_model = model.eval() # Use the model from the training cell\n",
        "        else:\n",
        "             raise RuntimeError(\"No trained model checkpoint found or loaded.\")\n",
        "else:\n",
        "    print(f\"ERROR: Model checkpoint not found at {SAVE_PATH}\")\n",
        "    if 'model' in globals():\n",
        "        print(\"Using model trained in the current session.\")\n",
        "        loaded_model = model.eval() # Use the model from the training cell\n",
        "    else:\n",
        "        raise RuntimeError(\"No trained model checkpoint found or loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45f33310",
      "metadata": {
        "id": "45f33310",
        "outputId": "992681b5-7615-4cd3-9d6e-004a1ca68bc9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                              \r"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x400 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# -----------------------------------------------------------\n",
        "# Cell 13 – Generate & display all digits 0–9 with DDIM-fast\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "# 1) Prepare figure with 2 rows × 5 columns\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
        "fig.suptitle(\"DDIM‐Fast Generated MNIST Digits 0–9\", fontsize=16)\n",
        "\n",
        "# 2) Loop over target digits\n",
        "for digit in range(10):\n",
        "    # 2a) Create class‐condition tensor\n",
        "    y_cond = torch.tensor([digit], device=DEVICE, dtype=torch.long)\n",
        "    \n",
        "    # 2b) Sample one image\n",
        "    with torch.no_grad():\n",
        "        sample = sample_ddim_fast(loaded_model,\n",
        "                                  y_cond,\n",
        "                                  num_steps=DDIM_STEPS,\n",
        "                                  eta=DDIM_ETA)   # → [1,1,32,32]\n",
        "    \n",
        "    # 2c) Clamp & normalize from [-1,1] → [0,1]\n",
        "    img = sample.clamp(-1, 1).cpu() * 0.5 + 0.5\n",
        "    img = img.squeeze()  # → [32,32]\n",
        "    \n",
        "    # 2d) Pick the right subplot\n",
        "    ax = axes[digit // 5, digit % 5]\n",
        "    ax.imshow(img.numpy(), cmap=\"gray\", vmin=0, vmax=1)\n",
        "    ax.set_title(f\"{digit}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f7718d7",
      "metadata": {
        "id": "9f7718d7",
        "outputId": "b77e66e7-c124-4111-a9a7-800fa21967ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                        "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier on real test images  →  acc=0.9890  F1=0.9890\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "# --------------------------------------\n",
        "# Cell 14 – Final test on real data \n",
        "# --------------------------------------\n",
        "\n",
        "# Ensure classifier is in eval mode\n",
        "clf.eval()\n",
        "\n",
        "# Evaluate the trained classifier on the *real* MNIST test set\n",
        "test_preds, test_labels = [], []\n",
        "with torch.no_grad():\n",
        "    # Use test_loader which uses the same transformations as training\n",
        "    for xb, yb in tqdm(test_loader, desc=\"Evaluating classifier on real test data\", leave=False):\n",
        "        xb = xb.to(DEVICE) # Move data to device\n",
        "        logits = clf(xb).cpu() # Get predictions on CPU\n",
        "        test_preds.append(logits.argmax(1))\n",
        "        test_labels.append(yb) # Keep original labels on CPU\n",
        "\n",
        "test_preds = torch.cat(test_preds).numpy()\n",
        "test_labels = torch.cat(test_labels).numpy()\n",
        "\n",
        "# Calculate overall metrics using macro averaging\n",
        "prec, rec, f1, _ = precision_recall_fscore_support(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "acc = accuracy_score(test_labels, test_preds)\n",
        "\n",
        "print(f\"Classifier on real test images  →  acc={acc:.4f}  F1={f1:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
